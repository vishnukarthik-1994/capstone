{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bdfdad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b69cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(img, threshold, model):\n",
    "    lab = {0:'both', 1:'infection', 2:'ischaemia', 3:'none'}\n",
    "    pred = model(img) # Pass the image to the model\n",
    "#     img = Image.open(img_path) # Load the image\n",
    "#    pred = model(img) # Pass the image to the model\n",
    "    pred_class = [lab[i] for i in list(pred[0]['labels'].cpu().numpy())] # Get the Prediction Score\n",
    "    print(pred_class)\n",
    "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].cpu().detach().numpy())] # Bounding boxes\n",
    "    print(pred_boxes)\n",
    "    pred_score = list(pred[0]['scores'].cpu().detach().numpy())\n",
    "    print(pred_score)\n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1] # Get list of index with score greater than threshold.\n",
    "    print(pred_t)\n",
    "    pred_boxes = pred_boxes[:pred_t+1]\n",
    "    print(pred_boxes)\n",
    "    pred_class = pred_class[:pred_t+1]\n",
    "    print(pred_class)\n",
    "    return pred_boxes, pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8de5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_img(im, threshold, model, rect_th=3, text_size=1, text_th=2):\n",
    "    boxes, pred_cls = get_prediction(im, threshold, model) # Get predictions\n",
    "    img = cv2.cvtColor(im.cpu().detach().numpy(), cv2.COLOR_BGR2RGB) # Convert to RGB\n",
    "    for i in range(len(boxes)):\n",
    "        img = cv2.rectangle(img, (int(boxes[i][0][0]), int(boxes[i][0][1])), (int(boxes[i][1][0]), int(boxes[i][1][1])),color=(0, 255, 0), thickness=rect_th) # Draw Rectangle with the coordinates\n",
    "        img = cv2.putText(img,pred_cls[i], (int(boxes[i][0][0]), int(boxes[i][0][1])),  cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th) # Write the prediction class\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e000a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karthik\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['infection', 'infection', 'infection', 'infection', 'none', 'none', 'infection', 'ischaemia', 'ischaemia', 'ischaemia', 'none', 'none', 'ischaemia', 'infection', 'none', 'infection', 'ischaemia', 'none', 'infection', 'ischaemia', 'none', 'infection', 'ischaemia', 'ischaemia', 'ischaemia', 'none', 'ischaemia', 'none', 'infection', 'ischaemia', 'none', 'ischaemia', 'ischaemia', 'ischaemia', 'none', 'infection', 'none', 'infection', 'ischaemia', 'ischaemia', 'none', 'ischaemia', 'none', 'infection', 'ischaemia', 'infection', 'ischaemia', 'none', 'ischaemia', 'infection', 'infection', 'none', 'none', 'none', 'none', 'infection', 'infection', 'none', 'infection']\n",
      "[[(16.793747, 9.349016), (71.07551, 81.5375)], [(151.64581, 76.804794), (213.1229, 165.19559)], [(25.124971, 14.58132), (66.48926, 61.875618)], [(127.3928, 63.558464), (224.0, 195.90312)], [(28.295216, 13.058758), (65.11158, 71.88756)], [(165.31068, 79.72174), (205.9932, 139.59286)], [(163.89629, 75.60928), (207.79489, 134.90901)], [(26.713203, 15.579405), (64.08234, 59.216244)], [(163.26454, 78.89999), (204.7895, 141.07393)], [(5.0499635, 14.836657), (73.7948, 70.57933)], [(90.194725, 0.0), (157.03973, 15.837286)], [(187.75948, 179.34598), (208.53587, 199.67186)], [(88.86607, 0.0), (157.0756, 16.186178)], [(86.71489, 0.0), (158.40834, 16.410114)], [(3.438682, 25.566088), (20.710657, 51.688225)], [(193.17535, 17.286673), (223.05487, 68.92327)], [(193.56024, 17.119522), (222.81613, 67.45771)], [(15.978697, 18.083755), (76.86684, 53.063923)], [(186.7369, 179.20186), (208.97478, 200.87103)], [(187.00684, 178.84889), (208.67836, 200.26936)], [(195.20073, 14.089929), (224.0, 63.22039)], [(0.0, 16.36712), (100.81835, 72.79505)], [(154.33601, 85.76625), (213.1434, 164.68817)], [(164.5107, 72.681526), (201.52351, 114.82668)], [(2.7605364, 24.48813), (20.376926, 52.673367)], [(166.48894, 74.041824), (203.36604, 113.674644)], [(124.92953, 65.63116), (224.0, 191.0585)], [(184.14238, 172.23509), (211.8173, 204.35869)], [(2.1753278, 25.150555), (21.155376, 53.743626)], [(26.051697, 29.117989), (63.724155, 81.638016)], [(7.6675487, 6.5031767), (73.49133, 79.99783)], [(9.793297, 163.12677), (53.35061, 217.17247)], [(184.36438, 169.71112), (211.37604, 204.9463)], [(149.331, 79.897995), (215.75084, 122.28756)], [(187.4282, 184.25896), (210.43056, 193.48607)], [(147.07509, 95.04345), (214.54805, 140.57146)], [(10.576968, 165.16353), (53.379143, 214.6939)], [(145.8367, 110.91587), (217.17056, 175.46097)], [(186.07506, 184.76212), (208.59782, 194.5154)], [(78.01242, 167.44128), (113.72544, 198.72256)], [(78.83137, 168.75322), (112.987144, 197.82133)], [(2.2461908, 101.61427), (39.050877, 151.38263)], [(164.47746, 100.97821), (202.17703, 161.6109)], [(0.0, 139.77759), (52.684963, 220.29572)], [(148.58403, 96.08912), (213.55022, 137.45096)], [(62.35813, 0.0), (137.42679, 19.077623)], [(0.0, 96.46422), (39.136894, 196.29697)], [(153.19531, 80.359604), (216.84917, 121.02946)], [(62.132324, 0.0), (125.982506, 17.079725)], [(162.82164, 103.743866), (206.27422, 163.20584)], [(77.74737, 168.11696), (113.28788, 198.57117)], [(132.56175, 66.16055), (224.0, 189.35338)], [(2.5981753, 103.11265), (39.06017, 150.0203)], [(0.0, 101.270584), (39.079407, 193.51817)], [(193.7814, 183.49283), (206.23012, 195.70955)], [(185.5702, 184.89796), (209.36914, 195.02417)], [(27.223501, 31.021076), (61.592724, 82.59991)], [(6.602471, 31.559887), (19.33132, 45.378605)], [(5.761988, 173.48358), (52.828724, 217.64346)]]\n",
      "[0.49979952, 0.47405672, 0.3565545, 0.274252, 0.23823857, 0.23336321, 0.20955592, 0.20534258, 0.20490062, 0.1804449, 0.17644398, 0.1704204, 0.16528185, 0.15234953, 0.14533885, 0.14346066, 0.14202705, 0.13788138, 0.13783453, 0.13538529, 0.12986444, 0.12884262, 0.122948006, 0.12246304, 0.114320904, 0.11393187, 0.11391444, 0.09769564, 0.09292061, 0.08718506, 0.08599917, 0.0847301, 0.08176764, 0.08127562, 0.0801978, 0.07878253, 0.077591784, 0.07702546, 0.0739952, 0.07319302, 0.071779974, 0.07163412, 0.07113872, 0.07113211, 0.06841668, 0.06624114, 0.065888815, 0.0657347, 0.06411733, 0.063048504, 0.06127745, 0.061069664, 0.060692314, 0.05999017, 0.059392665, 0.058637846, 0.05717798, 0.05532381, 0.05240456]\n",
      "1\n",
      "[[(16.793747, 9.349016), (71.07551, 81.5375)], [(151.64581, 76.804794), (213.1229, 165.19559)]]\n",
      "['infection', 'infection']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "data_transform = transforms.Compose([transforms.ToTensor()])\n",
    "original_img = Image.open(r\"C:\\object_detection\\frcnn_medium_sample_2\\391.jpg\")\n",
    "original_img.show()\n",
    "img = data_transform(original_img)\n",
    "img = torch.unsqueeze(img, dim=0)\n",
    "\n",
    "trial = cv2.cvtColor(img.cpu().detach().numpy(), cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(trial)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# original_img = cv2.imread(r\"C:\\object_detection\\frcnn_medium_sample_2\\391.jpg\")\n",
    "# plt.imshow(original_img[:,:,::-1])\n",
    "# plt.show()\n",
    "# img_tensor = torch.from_numpy(original_img)\n",
    "# img_tensor = img_tensor.float()\n",
    "# img_tensor = torch.unsqueeze(img_tensor, dim=0)\n",
    "# plt.imshow(img_tensor.detach().numpy()[0,:,:, ::-1])\n",
    "# plt.show()\n",
    "\n",
    "PATH = './frcnn_net_evaluate.pth'\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 4)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    img = img.to(device)\n",
    "    img = detect_img(img, 0.4, model)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1d5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
